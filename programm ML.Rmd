---
title: "PRACTICAL MACHINE LEARNING COURSERA PROJECT"
author: "cedric gaillard"
date: "Wednesday, October 21, 2015"
output: html_document
---


The goal of this project is to “predict the manner in which they did the exercise.”

For this project, I use the library following :
- Caret
- e1071
- ggplot2
- RandomForest
- rmarkdown

Five diffrents ways, as described in the study, were according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.

We use the following study:

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

We want predicted the appropriate activity quality with this 5 classes by processing data collected by accelerometers on the belt, forearm, arm, and dumbell of the participants with machine learning technical.

1. IMPORT DATA

We import the training and the test data in stating the decimals according regional setting.
#import data
> df_training <- read.csv2("C:/Users/Cedric/Downloads/pml-training.csv", sep=",",dec=".",na.strings=c("NA",""), header=TRUE)
> df_testing <- read.csv2("C:/Users/Cedric/Downloads/pml-training.csv", sep=",",dec=".",na.strings=c("NA",""), header=TRUE)

As there is many blanks,I decided to remplace this cells by "NA"" in importing the data with the option na.strings=c("NA","").

2. CHOICE OF FEATURES

We note there are many NA values. We count the number by colums :
#Count of NA
> NumberNA <- function(x) {as.vector(apply(x, 2, function(x) length(which(is.na(x)))))}
> colNA=NumberNA(df_training)

101 colums have 19216 values "NA". Many of this column are some calculated fields. So we can try to eliminate this columns of the analysis.

#Build of the dataset

> colnames_training=colnames(df_training)
> colnames_testing=colnames(df_testing)
> newcol=c()
> for (i in 1:length(df_training)) {if (colNA[i] == 0) {newcol= c(newcol,colnames_training[i])}}
> df_training <- df_training[,(names(df_training) %in% newcol)]

We apply the same traitement processing for the data test

> df_testing <- df_testing[,(names(df_testing) %in% newcol)]

Finally, we exclude the 7 first colums of the 2 datasets. They correspond to identification varaibles and they haven't explanatory power.

> df_training <- df_training[,8:length(df_training)]
> df_testing <- df_testing[,8:length(df_testing)]

> colnames(df_training)
 [1] "roll_belt"            "pitch_belt"           "yaw_belt"             "total_accel_belt"    
 [5] "gyros_belt_x"         "gyros_belt_y"         "gyros_belt_z"         "accel_belt_x"        
 [9] "accel_belt_y"         "accel_belt_z"         "magnet_belt_x"        "magnet_belt_y"       
[13] "magnet_belt_z"        "roll_arm"             "pitch_arm"            "yaw_arm"             
[17] "total_accel_arm"      "gyros_arm_x"          "gyros_arm_y"          "gyros_arm_z"         
[21] "accel_arm_x"          "accel_arm_y"          "accel_arm_z"          "magnet_arm_x"        
[25] "magnet_arm_y"         "magnet_arm_z"         "roll_dumbbell"        "pitch_dumbbell"      
[29] "yaw_dumbbell"         "total_accel_dumbbell" "gyros_dumbbell_x"     "gyros_dumbbell_y"    
[33] "gyros_dumbbell_z"     "accel_dumbbell_x"     "accel_dumbbell_y"     "accel_dumbbell_z"    
[37] "magnet_dumbbell_x"    "magnet_dumbbell_y"    "magnet_dumbbell_z"    "roll_forearm"        
[41] "pitch_forearm"        "yaw_forearm"          "total_accel_forearm"  "gyros_forearm_x"     
[45] "gyros_forearm_y"      "gyros_forearm_z"      "accel_forearm_x"      "accel_forearm_y"     
[49] "accel_forearm_z"      "magnet_forearm_x"     "magnet_forearm_y"     "magnet_forearm_z"    
[53] "classe"    

Except the variable income "classe", the dataset testing have the same variables.
After the cross-validation, we will verify if somme variables are constant in each sample.

3. PRE-PROCESSING AND ALGORYTHM

With 19622 observations, the processus could be very long. Furthermore, the data test is very small (20 observations). I divised the actual Training set in a new training set (60%) and a new testing set.
# build small sample for cross-validation

> inTrain <- createDataPartition(y=df_training$classe, p=0.6, list=FALSE)
> df_small_training <- df_training[inTrain,]
> df_small_testing <- df_training[-inTrain,]

We verify on this 2 samples if some variables are constant altought the sample stay very large.

> nsv <- nearZeroVar(df_small_training, saveMetrics=TRUE)

The test is False for all the variables in this sample.
Idem for the small testing sample.
The income variable is categorial so we test the random forest model which combines which combines the concepts of random in space and bagging.

# With standardisation
> set.seed(1000)
> modFit <- train(df_small_training$classe ~ ., method="rf", preProcess=c("center", "scale"), data=df_small_training)

Random Forest 

11776 samples
   52 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

Pre-processing: centered (52), scaled (52) 
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 11776, 11776, 11776, 11776, 11776, 11776, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
   2    0.9854208  0.9815414  0.001882687  0.002376889
  27    0.9858791  0.9821224  0.001674985  0.002113770
  52    0.9758053  0.9693644  0.004861828  0.006169726

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 27. 


4. RESULTS

# Results on the small data testing
> predictions <- predict(modFit, newdata=df_small_testing)
> confusionMatrix(predictions, df_small_testing$classe)

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 2224   15    0    0    0
         B    8 1494   12    1    2
         C    0    9 1351   17    3
         D    0    0    5 1268   10
         E    0    0    0    0 1427

Overall Statistics
                                         
               Accuracy : 0.9895         
                 95% CI : (0.987, 0.9917)
    No Information Rate : 0.2845         
    P-Value [Acc > NIR] : < 2.2e-16      
                                         
                  Kappa : 0.9868         
 Mcnemar's Test P-Value : NA             

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9964   0.9842   0.9876   0.9860   0.9896
Specificity            0.9973   0.9964   0.9955   0.9977   1.0000
Pos Pred Value         0.9933   0.9848   0.9790   0.9883   1.0000
Neg Pred Value         0.9986   0.9962   0.9974   0.9973   0.9977
Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2835   0.1904   0.1722   0.1616   0.1819
Detection Prevalence   0.2854   0.1933   0.1759   0.1635   0.1819
Balanced Accuracy      0.9969   0.9903   0.9915   0.9919   0.9948


With out of sample of 1.005%, we can apply this model to the 20 testing set.


# FINAL RESULTS
predict(modFit, newdata=df_testing)

[1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E

